{"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8980505,"sourceType":"datasetVersion","datasetId":5407711}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport scipy.io as sio\n\nAD = sio.loadmat(\"/kaggle/input/eeg-dataset/AD.mat\")\nMCI = sio.loadmat(\"/kaggle/input/eeg-dataset/MCI.mat\")\nNormal = sio.loadmat(\"/kaggle/input/eeg-dataset/normal.mat\")","metadata":{"id":"0ve5Cc7pBw8w","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"AD","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h3LTTgUTEkvQ","outputId":"6948192b-02ae-40d6-9484-1d509366e313","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nAD_Data = []\nfor i in range(len(AD['AD'][0])):\n  a = AD['AD'][0][i][0]\n  data = np.transpose(a, (2, 0, 1))\n  for j in range(data.shape[0]):\n    AD_Data.append(data[j])\n\nAD_Data = np.array(AD_Data)\nAD_Data.shape","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a3zhN5kJEl1m","outputId":"7efb4ac1-4a6d-4ff8-a859-f03435ca780a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"AD_Data","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8qkPqihtVatf","outputId":"78a1c231-9011-4c36-d3fa-cb07159364a6","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MCI_Data = []\nfor i in range(len(MCI['MCI'][0])):\n  a = MCI['MCI'][0][i][0]\n  data = np.transpose(a, (2, 0, 1))\n  for j in range(data.shape[0]):\n    MCI_Data.append(data[j])\n\nMCI_Data = np.array(MCI_Data)\nMCI_Data.shape","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ShEdraqIJuSW","outputId":"e46b2038-d6f8-463b-a2f8-0687a98e8675","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Normal_Data = []\nfor i in range(len(Normal['normal'][0])):\n  a = Normal['normal'][0][i][0]\n  data = np.transpose(a, (2, 0, 1))\n  for j in range(data.shape[0]):\n    Normal_Data.append(data[j])\n\nNormal_Data = np.array(Normal_Data)\nNormal_Data.shape","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UdxXjHJTJx93","outputId":"10e5d472-d58a-46d6-d3f2-164372cbd7d4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Data = []\nlabel = []\nfor i in range(Normal_Data.shape[0]):\n  Data.append(Normal_Data[i])\n  label.append(0)\n\nfor j in range(MCI_Data.shape[0]):\n  Data.append(MCI_Data[j])\n  label.append(1)\n\nfor k in range(AD_Data.shape[0]):\n  Data.append(AD_Data[k])\n  label.append(2)","metadata":{"id":"rs7Lx8mhWa0x","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Data = np.array(Data)\nlabel = np.array(label)\nprint(Data.shape)\nprint(label.shape)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tK3mmUEaXEv6","outputId":"5f81844b-5573-4f62-ff4f-5a9ab23989b9","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Ndata=[]\nfor i in  range(len(Data)):\n    std=np.std(Data[i],axis=1)\n    mean=np.mean(Data[i],axis=1)\n    Data[i]=(Data[i].transpose()-mean.transpose()).transpose()\n    Data[i]=(Data[i].transpose()/std.transpose()).transpose()\n    Ndata.append(Data[i])","metadata":{"id":"4glvNex6XFtE","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Ndata","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9aIubiBMyHc2","outputId":"590b5b85-d8a3-46a2-c956-fe1d1c9b355b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from matplotlib import pyplot as plt\nplt.plot(Ndata[0][0])","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":448},"id":"FQFHMKUN7zQV","outputId":"2fdb1984-9f8d-417b-8675-8fec01753d67","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.utils import to_categorical\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import train_test_split\n\nnum_classes = 3\nY = to_categorical(label, num_classes)\nx,y = shuffle(Data, Y, random_state = 2)\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.1, random_state = 2)\nprint(\"Shape of X_train:\", X_train.shape)\nprint(\"Shape of X_test:\", X_test.shape)\nprint(\"Shape of y_train:\", y_train.shape)\nprint(\"Shape of y_test:\", y_test.shape)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IjKUkfakXLDy","outputId":"d2cb1ef9-9441-4175-cd2e-a58579439d0d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reshape the input data\nX_train = X_train.reshape((3489, 600, 4))","metadata":{"id":"vLqyUazmVube","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.shape","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wlYAnVBUWtxf","outputId":"3a78b152-6423-4961-f41f-a8daafb0d88a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train.shape","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"balpbbMlZuip","outputId":"171bc246-95bf-4a8d-caf8-b2dac19c76ad","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Augmentation","metadata":{"id":"eRgp5yzQtnPZ"}},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import confusion_matrix\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, BatchNormalization, Dense, Dropout\nfrom tensorflow.keras.optimizers import Adam\n\ndef sliding_window_augmentation(X, y, window_size, step_size):\n    n_samples, n_timesteps, n_features = X.shape\n    augmented_X = []\n    augmented_y = []\n\n    for sample, label in zip(X, y):\n        for start in range(0, n_timesteps - window_size + 1, step_size):\n            window = sample[start:start+window_size]\n            augmented_X.append(window)\n            augmented_y.append(label)\n\n    return np.array(augmented_X), np.array(augmented_y)\n\n# Perform data augmentation\nwindow_size = int(600 * 0.75)  # 75% of original size w\nstep_size = 4  # distance of sliding window n\n\nX_augmented, y_augmented = sliding_window_augmentation(X_train, y_train, window_size, step_size)\n\nprint(f\"Augmented X shape: {X_augmented.shape}\")\nprint(f\"Augmented y shape: {y_augmented.shape}\")","metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"iGKZDLwl-o6e","outputId":"a8702da1-0f61-48fd-ae47-6b62a998cac8","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_augmented","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Model with 5-Fold Cross Validation**","metadata":{}},{"cell_type":"code","source":"# Building the model layers\ndef create_model(input_shape, num_classes):\n    model = Sequential([\n        Conv1D(24, kernel_size=3, activation='leaky_relu', input_shape=input_shape),\n        BatchNormalization(),\n        Conv1D(16, kernel_size=3, activation='leaky_relu'),\n        BatchNormalization(),\n        Conv1D(8, kernel_size=3, activation='leaky_relu'),\n        BatchNormalization(),\n        Conv1D(4, kernel_size=3, activation='leaky_relu'),\n        BatchNormalization(),\n        Flatten(),\n        Dense(64, activation='leaky_relu'),\n        BatchNormalization(),\n        Dropout(0.2),\n        Dense(num_classes, activation='softmax')\n    ])\n    optimizer = Adam(learning_rate=0.00002)\n    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n    return model\n\n# Perform 5-fold cross-validation\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\nfold = 1\nconfusion_matrices = []\n\nfor train_index, test_index in kf.split(X_augmented):\n    print(f\"Fold {fold}\")\n\n    X_train_fold, X_test_fold = X_augmented[train_index], X_augmented[test_index]\n    y_train_fold, y_test_fold = y_augmented[train_index], y_augmented[test_index]\n\n    model = create_model(input_shape=(X_train_fold.shape[1], X_train_fold.shape[2]), num_classes=3)\n\n    # Train the model\n    model.fit(X_train_fold, y_train_fold, epochs=20, batch_size=16, validation_split=0.2, verbose=1)\n\n    # Evaluate the model\n    y_pred = model.predict(X_test_fold)\n    y_pred_classes = np.argmax(y_pred, axis=1)\n    y_true_classes = np.argmax(y_test_fold, axis=1)\n\n    # Create confusion matrix\n    cm = confusion_matrix(y_true_classes, y_pred_classes)\n    confusion_matrices.append(cm)\n\n    print(f\"Confusion Matrix for Fold {fold}:\")\n    print(cm)\n    print(\"\\n\")\n\n    fold += 1\n\n# Calculate and print the average confusion matrix\navg_cm = np.mean(confusion_matrices, axis=0)\nprint(\"Average Confusion Matrix:\")\nprint(avg_cm)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\n\n# Initialize lists to store ROC curve data and AUC values\nfpr_list = []\ntpr_list = []\nroc_auc_list = []\n\n# Perform 5-fold cross-validation\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\nfold = 1\n\nfor train_index, test_index in kf.split(X_augmented):\n    print(f\"Fold {fold}\")\n\n    X_train_fold, X_test_fold = X_augmented[train_index], X_augmented[test_index]\n    y_train_fold, y_test_fold = y_augmented[train_index], y_augmented[test_index]\n\n    model = create_model(input_shape=(X_train_fold.shape[1], X_train_fold.shape[2]), num_classes=3)\n\n    # Train the model\n    model.fit(X_train_fold, y_train_fold, epochs=20, batch_size=16, validation_split=0.2, verbose=1)\n\n    # Evaluate the model\n    y_pred = model.predict(X_test_fold)\n\n    # Compute ROC curve and AUC for each class\n    n_classes = y_test_fold.shape[1]\n    fpr = dict()\n    tpr = dict()\n    roc_auc = dict()\n\n    for i in range(n_classes):\n        fpr[i], tpr[i], _ = roc_curve(y_test_fold[:, i], y_pred[:, i])\n        roc_auc[i] = auc(fpr[i], tpr[i])\n\n    # Store the ROC curve data and AUC values\n    fpr_list.append(fpr)\n    tpr_list.append(tpr)\n    roc_auc_list.append(roc_auc)\n\n    # Plot ROC curves for this fold\n    plt.figure()\n    colors = ['blue', 'red', 'green']\n    for i, color in zip(range(n_classes), colors):\n        plt.plot(fpr[i], tpr[i], color=color, lw=2,\n                 label='ROC curve of class {0} (area = {1:0.2f})'.format(i, roc_auc[i]))\n    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title(f'ROC Curve for Fold {fold}')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    fold += 1\n\n# Calculate and print the average AUC values\navg_auc = {i: np.mean([roc_auc_list[j][i] for j in range(5)]) for i in range(n_classes)}\nprint(\"Average AUC Values:\")\nprint(avg_auc)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Performance Metrics","metadata":{"id":"sdeJb_0qQzta"}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\ndef calculate_metrics(y_true, y_pred):\n    return {\n        'Accuracy': accuracy_score(y_true, y_pred),\n        'Precision': precision_score(y_true, y_pred, average='weighted'),\n        'Recall': recall_score(y_true, y_pred, average='weighted'),\n        'F1-score': f1_score(y_true, y_pred, average='weighted')\n    }\n\n# Initialize lists to store metrics for each fold\nfold_metrics = []\n\n# Calculate metrics for each fold\nfor fold, (train_index, test_index) in enumerate(kf.split(X_augmented), 1):\n    X_test_fold = X_augmented[test_index]\n    y_test_fold = y_augmented[test_index]\n    \n    # Get the model predictions (assuming you've saved the models from each fold)\n    y_pred = model.predict(X_test_fold)\n    y_pred_classes = np.argmax(y_pred, axis=1)\n    y_true_classes = np.argmax(y_test_fold, axis=1)\n    \n    # Calculate metrics\n    metrics = calculate_metrics(y_true_classes, y_pred_classes)\n    metrics['Fold'] = fold\n    fold_metrics.append(metrics)\n\n# Create a DataFrame with the results\nresults_df = pd.DataFrame(fold_metrics)\n\n# Calculate mean and standard deviation of metrics across folds\nmean_metrics = results_df.mean().round(3)\nstd_metrics = results_df.std().round(3)\n\n# Add mean and std to the results DataFrame\nresults_df = pd.concat([results_df, \n                        pd.DataFrame([mean_metrics], index=['Mean']),\n                        pd.DataFrame([std_metrics], index=['Std'])\n                       ]).reset_index()\n\n# Rename the 'index' column to 'Fold' for the Mean and Std rows\nresults_df.loc[results_df['index'].isin(['Mean', 'Std']), 'Fold'] = results_df['index']\nresults_df = results_df.drop('index', axis=1)\n\n# Reorder columns to put 'Fold' first\ncols = ['Fold'] + [col for col in results_df.columns if col != 'Fold']\nresults_df = results_df[cols]\n\n# Display the results\nprint(\"Performance Metrics for Each Fold:\")\nprint(results_df.to_string(index=False))\n\n# Calculate and display overall metrics\nprint(\"\\nOverall Performance:\")\noverall_metrics = calculate_metrics(\n    np.argmax(y_augmented, axis=1),\n    np.argmax(model.predict(X_augmented), axis=1)\n)\nfor metric, value in overall_metrics.items():\n    print(f\"{metric}: {value:.3f}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Graphs and Plots","metadata":{"id":"Fp1QU4XgQ7gm"}},{"cell_type":"code","source":"# Matplotlib CM\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Assuming you have 3 classes\nclass_names = ['AD', 'MCI', 'HC']  # Replace with your actual class names\n\n# Set up the matplotlib figure\nfig, axes = plt.subplots(2, 3, figsize=(15, 10))\nfig.suptitle('Confusion Matrices for Each Fold', fontsize=16)\n\n# Flatten the axes array for easier indexing\naxes = axes.flatten()\n\n# Plot confusion matrix for each fold\nfor i, cm in enumerate(confusion_matrices):\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names, ax=axes[i])\n    axes[i].set_title(f'Fold {i+1}')\n    axes[i].set_ylabel('True label')\n    axes[i].set_xlabel('Predicted label')\n\n# Plot average confusion matrix\nsns.heatmap(avg_cm, annot=True, fmt='.2f', cmap='Blues', xticklabels=class_names, yticklabels=class_names, ax=axes[-1])\naxes[-1].set_title('Average')\naxes[-1].set_ylabel('True label')\naxes[-1].set_xlabel('Predicted label')\n\n# Remove the last (empty) subplot\nfig.delaxes(axes[5])\n\n# Adjust the layout and display the plot\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}